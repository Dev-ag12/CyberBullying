

# **Problem Statement**

The rise of social media has brought about a concerning increase in cyberbullying, necessitating effective detection methods. This project aims to design a deep learning model to identify cyberbullying in social media posts. Deep learning offers promise due to its ability to process complex textual data. The report outlines the literature survey, and dataset selection for the project.

# **Solution strategy**

The solution strategy for this problem involves leveraging deep learning techniques to develop a robust model capable of accurately detecting instances of cyberbullying in social media posts.  
Firstly, a comprehensive survey is conducted to understand existing methodologies and algorithms used in cyberbullying detection.  
Initially, the dataset undergoes preprocessing steps to standardize the text, remove stop words, and handle issues like special characters. Subsequently, various embedding techniques such as word embeddings (e.g., Word2Vec, GloVe), contextual embeddings (e.g., BERT,ALBERT), and custom embeddings using Autoencoders are generated from the preprocessed text data. These embeddings capture semantic and contextual information essential for understanding the nuances of social media language.

Following embedding generation, different deep learning models are trained and evaluated using these embeddings. Models like Long short term memory(LSTMs), Artificial neural networks (ANNs), and pretrained models like BERT. The choice of model depends on factors like the complexity of the dataset, computational resources, and the desired balance between interpretability and performance.

Hyperparameter tuning is conducted to optimize model performance. Finally, the trained models are evaluated using appropriate evaluation metrics to assess their effectiveness in discriminating between cyberbullying and non-cyberbullying instances in social media posts. The most suitable model is then selected based on its performance metrics and overall efficiency in cyberbullying detection.

# **Dataset**

[https://www.kaggle.com/datasets/saurabhshahane/cyberbullying-dataset?select=twitter\_parsed\_d](https://www.kaggle.com/datasets/saurabhshahane/cyberbullying-dataset?select=twitter\_parsed\_dataset.csv) [ataset.csv](https://www.kaggle.com/datasets/saurabhshahane/cyberbullying-dataset?select=twitter\_parsed\_dataset.csv)

# **Dataset Description**

The dataset provided is a comprehensive collection of data sourced from the social media platform Twitter. It is curated specifically for the purpose of automatic detection of cyberbullying, a pervasive issue in online communities.  
The dataset comprises textual data along with corresponding labels indicating whether the content is categorized as cyberbullying or not. Each entry in the dataset is annotated with labels that distinguish between instances of cyberbullying and non-bullying content.  
Notably, the dataset covers a wide range of cyberbullying behaviors, including but not limited to racism and sexism. This diversity enables the development of models capable of identifying various forms of cyberbullying, catering to the nuanced nature of online harassment.  
In summary, this dataset serves as a valuable resource aiming to address the pressing issue of cyberbullying through data-driven approaches. Its comprehensive coverage of different types of cyberbullying behaviors makes it a valuable asset for training and evaluating machine learning models and algorithms for cyberbullying detection.

# **Major Innovations**

## **Custom Autoencoder Architecture to generate word embeddings**

The Autoencoder architecture implemented by us serves as a robust framework for generating sentence embeddings from preprocessed textual data. It comprises several interconnected components designed to efficiently encode and decode sequences of word indices representing sentences.

At its core lies an embedding layer, which transforms the discrete word indices into dense vector representations of fixed dimensions.  
Following the embedding layer, the architecture employs a bidirectional LSTM as the encoder, responsible for processing the embedded sequences in both forward and backward directions. This bidirectional nature enables the model to capture contextual dependencies and semantic relationships within the input sentences comprehensively.

The decoder LSTM, which operates bidirectionally as well, utilizes the encoded representations generated by the encoder to reconstruct the input sequences. Through iterative decoding, the decoder LSTM refines the initial representations to generate coherent outputs that closely resemble the original input sentences.

To facilitate the reconstruction process, the decoder LSTM's outputs are passed through a linear layer, which maps the hidden representations to the vocabulary size. This output layer serves to align the model's predictions with the actual word indices, facilitating the reconstruction of the input sentences.  
By minimizing the reconstruction error between the predicted outputs and the original input sequences during training, the model learns to generate embeddings that faithfully capture the underlying semantic content of the textual data.

In summary, by leveraging bidirectional LSTMs and embedding layers, the model aptly captures the semantic intricacies of input sentences, thereby enabling a wide range of downstream applications such as text classification, clustering, and information retrieval.

## **Custom loss function**

The provided code snippet defines a custom loss function, CustomLossWithFocal, which combines the Focal Loss with Entropy loss for text classification tasks(BCE for binary and CrossEntropy for multi-class). Focal Loss is designed to address the class imbalance problem by focusing more on hard-to-classify examples, thereby mitigating the impact of dominant classes. It introduces two hyperparameters, alpha and gamma, which control the balance between the easy and hard examples and the rate at which the loss penalizes misclassifications, respectively.

In the CustomLossWithFocal function, the BCE loss is first calculated which measures the difference between the predicted logits and the target labels. Then, the Focal loss is computed by incorporating the alpha and gamma parameters, adjusting the loss values for each example based on their difficulty in classification.

Where

Pt represents the probability predicted by the model for the true class.

α is a hyperparameter that balances the importance of easy and hard examples. It assigns higher weights to misclassified hard examples.  
γ is another hyperparameter that controls the rate at which the loss penalizes well-classified examples. Higher values of γ lead to a sharper focus on misclassified examples.

Additionally, a regularization term is included to prevent overfitting by penalizing large parameter values in the model.

The combined loss function offers several benefits for text classification tasks:

\- Firstly, the Focal Loss component helps in handling imbalanced datasets commonly encountered in text classification scenarios, where certain classes may dominate the training data. By assigning higher weights to misclassified examples, it ensures that the model focuses more on learning from challenging instances, leading to improved generalization performance.

\-Secondly, the regularization term helps in controlling the complexity of the model, preventing it from fitting noise in the data and enhancing its ability to generalize to unseen examples.

Overall, the custom loss function provides a comprehensive approach to training deep learning models for text classification, addressing both class imbalance and overfitting issues effectively.

# **Results**

| Index | Model | Embedding/Tokens | Loss Used | Result |
| :---- | :---- | :---- | :---- | :---- |
| **1** | **BERT** | **BERT Tokenizer** | **Cross Entropy** | **85%** |
| **2** | **LSTM** | **ROBERT Tokenizer** | **Cross Entropy** | **83%** |
| **3** | **ANN** | **Embeddings from Autoencoder** | **Custom Loss** | **68%** |
| **4** | **ANN** | **Embeddings from Autoencoder** | **Cross Entropy** | **62%** |
| **5** | **LSTM** | **Glove Embeddings** | **Cross Entropy** | **81%** |
| **6** | **LSTM** | **Glove Embeddings** | **Custom Loss** | **69%** |

**Analysis and Conclusion**

1) ### **Pretrained BERT**

Here a pre-trained BERT model is used for sequence classification using the BERT tokenizer. and creating PyTorch datasets and DataLoaders for efficient training. The pre-trained BERT model for sequence classification is loaded and adapted for the specific classification task with three output labels.This implementation leverages BERT's contextualized embeddings and  
fine-tunes the model for the specific classification task, aiming to achieve high accuracy on the validation set.  
BERT utilizes attention mechanisms to focus on relevant parts of the input sequence, facilitating more efficient and accurate information processing. Overall, BERT's sophisticated architecture, extensive pre-training, and attention mechanisms contribute to its superior performance in various natural language processing tasks compared to other models such as LSTMs or autoencoders.

1) ### **LSTM with ROBERT Tokenizer**

Here Long Short-Term Memory (LSTM) model is used for text classification, utilizing the RoBERTa tokenizer for tokenization. The LSTM model architecture consists of an embedding layer, an LSTM layer, and a linear layer for classification. While the RoBERTa tokenizer is used for tokenization, the model itself is a custom LSTM, not leveraging RoBERTa's contextualized embeddings. This implementation serves as a foundational example of text classification using LSTMs but may lack the nuanced representations provided by pre-trained models like RoBERTa.

1) ### **ANN with embeddings from custom auto-encoder**

Here the text embedding extraction is done using a custom trained sequence-to-sequence autoencoder and subsequent classification using an artificial neural network (ANN). First, the trained autoencoder model is loaded and set to evaluation mode. Text data is converted into PyTorch tensors and passed through the encoder part of the autoencoder to extract embeddings. These embeddings are then converted into numpy arrays for downstream tasks. Subsequently, an ANN classifier model is defined and trained on the extracted embeddings. The classifier is then trained using cross-entropy loss & a custom loss function.  
It does not give a high accuracy as the custom embeddings utilized in the model are extracted from a sequence-to-sequence autoencoder, which may not capture contextual information as effectively as models like BERT. Unlike BERT, which learns representations from large-scale text corpora with bidirectional context, the autoencoder embeddings may lack the rich semantic understanding and contextualization required for accurate classification.

1) ### **LSTM with Glove Embeddings**

Here the word embeddings are generated using pre-trained GloVe embeddings. Initially, it reads the GloVe embeddings file and stores the word embeddings in a dictionary called embeddings\_index. It then initializes an embedding matrix based on the vocabulary from the dataset, where each word's embedding is retrieved from embeddings\_index and added to the matrix. Then an LSTM is trained using both the custom loss and the cross entropy loss . Overall, it demonstrates the integration of pre-trained word embeddings into a neural network training pipeline for natural language processing tasks and has achieved a decent accuracy of 81%.

In conclusion, the provided code snippets showcase various approaches to text classification using different models and techniques. BERT, with its bidirectional transformer architecture and pre-training on large text corpora, demonstrates superior performance compared to other models like artificial neural networks (ANNs) with custom embeddings or GloVe embeddings. BERT's ability to capture rich semantic meanings and contextual information contributes to its effectiveness in understanding and classifying text data accurately. Meanwhile, the ANN models, while capable of leveraging pre-trained embeddings, may suffer from limitations in capturing nuanced contextual information and optimizing model parameters effectively. Overall, the choice of model and technique depends on the specific requirements of the text classification task, including dataset size, complexity, and computational resources available. Further experimentation and tuning of hyperparameters may be necessary to achieve optimal performance for a given task.

# **References**

[https://medium.com/analytics-vidhya/word-vectorization-using-glove-76919685ee0b](https://medium.com/analytics-vidhya/word-vectorization-using-glove-76919685ee0b) [https://alvinntnu.github.io/python-notes/nlp/word-embeddings-autoencoder.html](https://alvinntnu.github.io/python-notes/nlp/word-embeddings-autoencoder.html)